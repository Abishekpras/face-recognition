{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from skimage import io\n",
    "from scipy.misc import imread\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pickle\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: `imread` is deprecated!\n",
      "`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imread`` instead.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "file_loc = 'lfw-deepfunneled//'\n",
    "\n",
    "ctr = 0\n",
    "idx = 0\n",
    "MIN_SAMPLES = 20\n",
    "name = [None]\n",
    "img = []\n",
    "label = []\n",
    "for root, dirs, files in os.walk(file_loc):\n",
    "    for file in files:\n",
    "        with open(os.path.join(root, file), \"r\") as img_data:\n",
    "            name.append(img_data.name)\n",
    "        idx+=1\n",
    "    if(idx>MIN_SAMPLES):\n",
    "        [(img.append(imread(x)),label.append(ctr)) for x in name if x != None]\n",
    "        ctr+=1\n",
    "    name = [None]\n",
    "    idx=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2923\n"
     ]
    }
   ],
   "source": [
    "nb_classes = max(label)+1\n",
    "print(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(label, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(label)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(57, 2923)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique), len(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0, 121],\n",
       "       [  1,  25],\n",
       "       [  2,  22],\n",
       "       [  3,  26],\n",
       "       [  4,  29],\n",
       "       [  5,  53],\n",
       "       [  6, 109],\n",
       "       [  7,  23],\n",
       "       [  8, 236],\n",
       "       [  9,  21]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = np.asarray((unique, counts)).T\n",
    "l[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fn(cls):\n",
    "    return list(set(range(len(img))) - set(cls))\n",
    "\n",
    "cls_gp = [None]*nb_classes\n",
    "cls_gp_bar = [None]*nb_classes\n",
    "cls_gp[0] = list(range(counts[0]))\n",
    "cls_gp_bar[0] = fn(cls_gp[0])\n",
    "\n",
    "for i in range(1,nb_classes):\n",
    "    cls_gp[i] = list(range(counts[i-1], counts[i-1]+counts[i]))\n",
    "    cls_gp_bar[i] = fn(cls_gp[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2923"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(img),np.array(label), train_size=0.9, random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2630, 250, 250, 3) (293, 250, 250, 3) 57\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "y_train = np.array(y_train) \n",
    "y_test = np.array(y_test)\n",
    "print(X_train.shape, X_test.shape, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2630,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train_test_dt_1.pkl', 'wb') as handle:\n",
    "    pickle.dump((X_train, X_test, y_train, y_test, nb_classes, img, label, unique, counts), handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('train_test_dt_1.pkl', 'rb') as handle:\n",
    "    X_train, X_test, y_train, y_test, nb_classes, img, label, unique, counts = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.python.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = to_categorical(y_train, nb_classes)\n",
    "Y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2630, 250, 250, 3) (2630, 57) (293, 250, 250, 3) (293, 57) 57\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import backend as K\n",
    "from keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 250, 250, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 250, 250, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 250, 250, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 125, 125, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 125, 125, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 125, 125, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 62, 62, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 62, 62, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 62, 62, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 62, 62, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 31, 31, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 31, 31, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 31, 31, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 15, 15, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 15, 15, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 128)               3211392   \n",
      "=================================================================\n",
      "Total params: 17,926,080\n",
      "Trainable params: 3,211,392\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"se...)`\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Flatten, Dense, Lambda\n",
    "from keras.models import Model\n",
    "from keras.engine.topology import Input\n",
    "\n",
    "\n",
    "input_tensor = Input(shape=img[0].shape)\n",
    "base_model = VGG16(weights='imagenet',include_top= False,input_tensor=input_tensor)\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable=False\n",
    "'''\n",
    "x = vgg_conv.layers[1](inp)\n",
    "for layer in vgg_conv.layers[1:]:\n",
    "    x = layer(x)\n",
    "'''\n",
    "top_model = models.Sequential()\n",
    "\n",
    "# Add new layers\n",
    "top_model.add(layers.Flatten(input_shape=base_model.output_shape[1:]))\n",
    "top_model.add(layers.Dense(128, activation='relu'))\n",
    "top_model.add(layers.Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer'))\n",
    "'''\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu', name='dense_emb_layer')\n",
    "x = Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer')\n",
    "\n",
    "model = Model(inputs=[inp], outputs=x)\n",
    "'''\n",
    "model = Model(input= base_model.input, output= top_model(base_model.output))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " \n",
    "# Create the model\n",
    "model = models.Sequential()\n",
    " \n",
    "# Add the vgg convolutional base model\n",
    "model.add(vgg_conv)\n",
    " \n",
    "# Add new layers\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Lambda(lambda  x: K.l2_normalize(x, axis=1), name='norm_layer'))\n",
    "#model.add(layers.Dense(nb_classes, activation='softmax'))\n",
    " \n",
    "# Show a summary of the model. Check the number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Layer\n",
    "\n",
    "# Input for anchor, positive and negative images\n",
    "in_a = Input(shape=img[0].shape)\n",
    "in_p = Input(shape=img[0].shape)\n",
    "in_n = Input(shape=img[0].shape)\n",
    "\n",
    "# Output for anchor, positive and negative embedding vectors\n",
    "# The nn4_small model instance is shared (Siamese network)\n",
    "emb_a = model(in_a)\n",
    "emb_p = model(in_p)\n",
    "emb_n = model(in_n)\n",
    "\n",
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        a, p, n = inputs\n",
    "        p_dist = K.sum(K.square(a-p), axis=-1)\n",
    "        n_dist = K.sum(K.square(a-n), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss\n",
    "\n",
    "# Layer that computes the triplet loss from anchor, positive and negative embedding vectors\n",
    "triplet_loss_layer = TripletLossLayer(alpha=0.5, name='triplet_loss_layer')([emb_a, emb_p, emb_n])\n",
    "\n",
    "# Model that can be trained with anchor, positive negative images\n",
    "train_model = Model([in_a, in_p, in_n], triplet_loss_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 250, 250, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            (None, 250, 250, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            (None, 250, 250, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "model_2 (Model)                 (None, 128)          17926080    input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "triplet_loss_layer (TripletLoss [(None, 128), (None, 0           model_2[1][0]                    \n",
      "                                                                 model_2[2][0]                    \n",
      "                                                                 model_2[3][0]                    \n",
      "==================================================================================================\n",
      "Total params: 17,926,080\n",
      "Trainable params: 3,211,392\n",
      "Non-trainable params: 14,714,688\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_cls_gp(set_img, set_label):\n",
    "    set_unique, set_counts = np.unique(set_label, return_counts=True)\n",
    "    def fn(cls):\n",
    "        return list(set(range(len(set_img))) - set(cls))\n",
    "    \n",
    "    set_cls_gp = [None]*nb_classes\n",
    "    set_cls_gp_bar = [None]*nb_classes\n",
    "    set_cls_gp[0] = list(range(set_counts[0]))\n",
    "    set_cls_gp_bar[0] = fn(set_cls_gp[0])\n",
    "\n",
    "    for i in range(1,nb_classes):\n",
    "        set_cls_gp[i] = list(range(set_counts[i-1], set_counts[i-1]+set_counts[i]))\n",
    "        set_cls_gp_bar[i] = fn(set_cls_gp[i])\n",
    "        \n",
    "    return (set_cls_gp, set_cls_gp_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def triplet_batch(set_img, set_label, batch_size = 32, seed = 42):\n",
    "    '''\n",
    "    sd = [(i,i,j) for i in range(nb_classes) for j in range(nb_classes) if (j!=i)]\n",
    "    ls = np.random.choice(np.arange(len(sd)), batch_size)\n",
    "    idc = [sd[x] for x in ls]\n",
    "    '''\n",
    "    assert len(set_img) == len(set_label)\n",
    "    set_cls_gp, set_cls_gp_bar = find_cls_gp(set_img, set_label)\n",
    "    #print('Max values set_grp {}, set_grp_bar {}, len {}'.format(max(set_cls_gp), max(set_cls_gp_bar), len(set_img)))\n",
    "    while True:\n",
    "        anc_in, pos_in, neg_in = [],[],[]\n",
    "        seed+=1\n",
    "        np.random.seed(seed)\n",
    "        lbl_iter = np.random.choice(range(nb_classes), batch_size, replace=True)\n",
    "        for t in lbl_iter:\n",
    "            q = set_cls_gp[t]\n",
    "            q_bar = set_cls_gp_bar[t]\n",
    "            \n",
    "            if (len(q) == 0):\n",
    "                raise Exception('Train set does not have images of label {}'.format(t))\n",
    "\n",
    "            anc,pos = np.random.choice(q,2,replace=False)\n",
    "            neg = int(np.random.choice(q_bar,1)[0])\n",
    "            \n",
    "            anc_in.append(anc)\n",
    "            pos_in.append(pos)\n",
    "            neg_in.append(neg)\n",
    "        anc_imgs = np.array([set_img[x] for x in anc_in])\n",
    "        pos_imgs = np.array([set_img[x] for x in pos_in])\n",
    "        neg_imgs = np.array([set_img[x] for x in neg_in])\n",
    "        yield ([anc_imgs,pos_imgs,neg_imgs], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_triplet_batch = triplet_batch(X_train,y_train)\n",
    "val_triplet_batch = triplet_batch(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object triplet_batch at 0x7f7e40163518>\n"
     ]
    }
   ],
   "source": [
    "print(train_triplet_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32, 250, 250, 3), (32, 250, 250, 3), (32, 250, 250, 3))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = next(train_triplet_batch)[0]\n",
    "sa[0].shape, sa[1].shape, sa[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(82, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "train_steps = (len(X_train)//batch_size)\n",
    "val_steps = (len(X_test)//batch_size)\n",
    "train_steps, val_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Output \"triplet_loss_layer\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"triplet_loss_layer\" during training.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 1050s 13s/step - loss: 8.0679 - val_loss: 3.2019\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 1043s 13s/step - loss: 2.9172 - val_loss: 2.3186\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 1043s 13s/step - loss: 2.1662 - val_loss: 2.3564\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 1042s 13s/step - loss: 1.7999 - val_loss: 1.7636\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 1041s 13s/step - loss: 1.4835 - val_loss: 1.3427\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 1045s 13s/step - loss: 1.4725 - val_loss: 0.8854\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 1044s 13s/step - loss: 1.2826 - val_loss: 1.0461\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 1043s 13s/step - loss: 1.2935 - val_loss: 0.9232\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 1044s 13s/step - loss: 1.0720 - val_loss: 0.9399\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 1043s 13s/step - loss: 1.0807 - val_loss: 0.7754\n"
     ]
    }
   ],
   "source": [
    "train_model.compile(loss=None, optimizer='adam')\n",
    "hist = train_model.fit_generator(train_triplet_batch, \n",
    "                          steps_per_epoch=train_steps,\n",
    "                          #steps_per_epoch=2,\n",
    "                          epochs=10,\n",
    "                          validation_data=train_triplet_batch,\n",
    "                          validation_steps=val_steps)\n",
    "                          #validation_steps=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEWCAYAAABliCz2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XmcVOWd7/HPr6r3pummF0BooQtRQFRAoMEYNW6JmsS4ZIxJdCbJnZDJK5ms4yTOZJnMnczNvZObidlMcJkkE0eTuNxJ1MQlomJUEHBFUAQaaNamoemN3qp+949TDd3I0ktVV3XV9/161atrOec8vyrle04956nnmLsjIiKZL5TqAkREZGQo8EVEsoQCX0QkSyjwRUSyhAJfRCRLKPBFRLKEAl8EMLOfm9m/DHDZOjO7ZLjbERlpCnwRkSyhwBcRyRIKfBk14l0pN5nZK2bWZmZ3mNkEM/uDmbWY2eNmNq7P8lea2VozazKzJ81sVp/X5pnZmvh6vwYKjmjrfWb2UnzdZ83srCHW/Ekze8vM9pnZ78xsUvx5M7N/N7M9ZtZsZq+a2Rnx164ws9fjtW03s78b0gcmcgQFvow21wKXAqcB7wf+APwDUEXw//PnAMzsNOBu4Avx1x4Gfm9meWaWB/w/4D+BcuC38e0SX3cecCfwKaAC+BnwOzPLH0yhZnYR8L+A64CTgC3APfGX3w2cH38fpfFlGuOv3QF8yt1LgDOAJwbTrsixKPBltPmhu+929+3AcmCFu7/o7h3AA8C8+HIfAh5y98fcvRv4LlAIvANYDOQC33f3bne/F3ihTxtLgJ+5+wp3j7r7L4DO+HqD8VHgTndf4+6dwM3AOWZWA3QDJcBMwNx9nbvvjK/XDZxuZmPdfb+7rxlkuyJHpcCX0WZ3n/sHj/J4TPz+JIIjagDcPQZsAybHX9vu/WcO3NLn/lTgy/HunCYzawJOjq83GEfW0EpwFD/Z3Z8AfgT8GNhjZkvNbGx80WuBK4AtZvaUmZ0zyHZFjkqBL5lqB0FwA0GfOUFobwd2ApPjz/Wa0uf+NuDb7l7W51bk7ncPs4Zigi6i7QDu/gN3nw+cTtC1c1P8+Rfc/QPAeIKup98Msl2Ro1LgS6b6DfBeM7vYzHKBLxN0yzwLPAf0AJ8zs1wzuwao7bPubcDfmNmi+MnVYjN7r5mVDLKGu4GPm9nceP//vxJ0QdWZ2cL49nOBNqADiMXPMXzUzErjXVHNQGwYn4PIIQp8yUju/gZwA/BDYC/BCd73u3uXu3cB1wAfA/YR9Pff32fdVcAnCbpc9gNvxZcdbA2PA18H7iP4VnEKcH385bEEO5b9BN0+jcC/xV+7Eagzs2bgbwjOBYgMm+kCKCIi2UFH+CIiWUKBLyKSJRT4IiJZQoEvIpIlclJdQF+VlZVeU1OT6jJEREaN1atX73X3qoEsm1aBX1NTw6pVq1JdhojIqGFmW068VEBdOiIiWUKBLyKSJRT4IiJZIq368I+mu7ub+vp6Ojo6Ul1KUhUUFFBdXU1ubm6qSxGRDJX2gV9fX09JSQk1NTX0n9wwc7g7jY2N1NfXE4lEUl2OiGSotO/S6ejooKKiImPDHsDMqKioyPhvMSKSWmkf+EBGh32vbHiPIpJaSQ18M/ti/CLSr5nZ3WZWcOK1BifmTkNLBy0d3YnetIhIRkla4JvZZIILSi9w9zOAMIfnAk9cO0BDSxdN7ckJ/KamJn7yk58Mer0rrriCpqamJFQkIjI0ye7SyQEKzSwHKCK45FtCmRnF+WHaunoSvWng2IHf03P89h5++GHKysqSUpOIyFAkLfDdfTvwXWArwdV+Drj7o0cuZ2ZLzGyVma1qaGgYUlvFeTl09cTo6kn8leC++tWvsnHjRubOncvChQs577zzuPLKKzn99NMBuOqqq5g/fz6zZ89m6dKlh9arqalh79691NXVMWvWLD75yU8ye/Zs3v3ud3Pw4MGE1ykiciJJG5ZpZuOADwARoAn4rZnd4O6/6rucuy8FlgIsWLDguJff+tbv1/L6jua3PR9z52BXlPzcMDmhwZ38PH3SWL75/tnHfP073/kOr732Gi+99BJPPvkk733ve3nttdcODZ+88847KS8v5+DBgyxcuJBrr72WioqKftvYsGEDd999N7fddhvXXXcd9913HzfccMOg6hQRGa5kdulcAmx294b4xZjvB96RjIZCZphBLJb8yzXW1tb2Gyv/gx/8gDlz5rB48WK2bdvGhg0b3rZOJBJh7ty5AMyfP5+6urqk1ykicqRk/vBqK7DYzIqAg8DFwLCmwjzekfjmvW109cSYMbFkOE2cUHFx8aH7Tz75JI8//jjPPfccRUVFvOtd7zrqWPr8/PxD98PhsLp0RCQlktmHvwK4F1gDvBpva+lxVxqG4vwwnT1RuqOJ7ccvKSmhpaXlqK8dOHCAcePGUVRUxPr163n++ecT2raISCIldWoFd/8m8M1kttGrOC94K+1dPZQW5iVsuxUVFZx77rmcccYZFBYWMmHChEOvXXbZZfz0pz9l1qxZzJgxg8WLFyesXRGRRDP35Pd7D9SCBQv8yAugrFu3jlmzZp1w3Zg7r+9oprw4j0llhckqMakG+l5FRHqZ2Wp3XzCQZUfF1AoDETKjKC9MW2dyxuOLiIx2GRP4AMX5ORzsjhKNJX48vojIaJdZgZ8XBqCtM5riSkRE0k9GBX5RXg5mlrRpFkRERrOMCvxQyCjMDesIX0TkKDIq8CEYj3+wKzoiv7oVERlNMjDwc3Cc9gR16wx1emSA73//+7S3tyekDhGR4cq8wM8LY0BbV2K6dRT4IpIp0v4i5oMVDoUoyE3cePy+0yNfeumljB8/nt/85jd0dnZy9dVX861vfYu2tjauu+466uvriUajfP3rX2f37t3s2LGDCy+8kMrKSpYtW5aQekREhmp0Bf4fvgq7Xj3hYlN6ovTEHM8LY5xguuSJZ8Ll3znmy32nR3700Ue59957WblyJe7OlVdeydNPP01DQwOTJk3ioYceAoI5dkpLS/ne977HsmXLqKysHNTbFBFJhozr0gEIhwx3SPR520cffZRHH32UefPmcfbZZ7N+/Xo2bNjAmWeeyWOPPcZXvvIVli9fTmlpaWIbFhFJgNF1hH+cI/F+ojE27WxmYmkB40sSd910d+fmm2/mU5/61NteW7NmDQ8//DBf+9rXuPjii/nGN76RsHZFRBIhI4/wc8IhCnISMx6/7/TI73nPe7jzzjtpbW0FYPv27ezZs4cdO3ZQVFTEDTfcwE033cSaNWvetq6ISKqNriP8QSjOD9PU3o27Yza4yx721Xd65Msvv5yPfOQjnHPOOQCMGTOGX/3qV7z11lvcdNNNhEIhcnNzufXWWwFYsmQJl112GZMmTdJJWxFJuYyZHvlITe1dbN3Xzqnjx1CYNzr2a5oeWUQGKyunRz5SUTzkWzXNgogIkMGBn5cTIi8nlLBf3IqIjHZJC3wzm2FmL/W5NZvZF4ayraF2OxXn5dDW2TPk9UfSaKhRREa3ZF7E/A13n+vuc4H5QDvwwGC3U1BQQGNj45ACsTg/h56Y09mT3hdEcXcaGxspKEjcEFIRkSON1NnMi4GN7r5lsCtWV1dTX19PQ0PDoBvticbY3dxJ595cxuSn94nbgoICqqurU12GiGSwkUrB64G7j/aCmS0BlgBMmTLlba/n5uYSiUSG1Ki789f/608silTwgw+fOaRtiIhkiqSftDWzPOBK4LdHe93dl7r7AndfUFVVlei2qY1UsHLzPvWRi0jWG4lROpcDa9x99wi09Ta1kXJ2NXewbd/BVDQvIpI2RiLwP8wxunNGwqJIOQArNjemqgQRkbSQ1MA3s2LgUuD+ZLZzPNOrxjCuKJeVm/elqgQRkbSQ1JO27t4GVCSzjRMJhYyFNeWsrFPgi0h2y9hf2vZVGylnS2M7uw50pLoUEZGUyYrAXxQJvmToKF9EsllWBP6sk0oYk5/DSp24FZEslhWBnxMOMX/qOJ24FZGslhWBD0E//pu7W9nX1pXqUkREUiJrAr93PP4L6scXkSyVNYF/ZnUp+TkhdeuISNbKmsDPzwkzb0qZAl9EslbWBD5AbaSCtTsO0NLRnepSRERGXFYF/qJIOTGH1Vv2p7oUEZERl1WBP29KGTkhU7eOiGSlrAr8orwczqwuVeCLSFbKqsCHYDz+y/VNdHRHU12KiMiIyrrAXxQppzvqvLi1KdWliIiMqKwL/PlTyzFD3ToiknWyLvBLC3OZNXEsK+s0kZqIZJesC3wI+vFXb9lPV08s1aWIiIyYrAz8RZFyOrpjvLbjQKpLEREZMcm+pm2Zmd1rZuvNbJ2ZnZPM9gZqYXwiNfXji0g2SfYR/i3AH919JjAHWJfk9gakckw+p1QVK/BFJKskLfDNrBQ4H7gDwN273D1txkLWRip4oW4f0ZinuhQRkRGRzCP8CNAA/IeZvWhmt5tZ8ZELmdkSM1tlZqsaGhqSWE5/iyLltHT0sH5X84i1KSKSSskM/BzgbOBWd58HtAFfPXIhd1/q7gvcfUFVVVUSy+mvVv34IpJlkhn49UC9u6+IP76XYAeQFiaVFVI9rlCBLyJZI2mB7+67gG1mNiP+1MXA68lqbyhqI+Ws3LwPd/Xji0jmS/Yonb8F7jKzV4C5wL8mub1BWRQpp7Gti40NbakuRUQk6XKSuXF3fwlYkMw2hqM2UgEE/fjTx49JcTUiIsmVlb+07VVTUURVST4rN2teHRHJfFkd+GZGbaScFerHF5EskNWBD0E//s4DHdTvP5jqUkREkirrA1/j8UUkW2R94J82voTSwlwFvohkvKwP/FDIWFhTzso6Bb6IZLasD3wI+vE3721jT3NHqksREUkaBT59+vF1lC8iGUyBD8yeNJaivLD68UUkoynwgZxwiPlTxynwRSSjKfDjFk+rYP2uFprau1JdiohIUijw43r78V+o25/iSkREkkOBH3dWdSl5OSHNqyMiGUuBH5efE2beyWXqxxeRjKXA72NRpJzXdjTT2tmT6lJERBJOgd9HbaSCaMxZs0X9+CKSeRT4fZw9tYyckKlbR0QykgK/j6K8HM6YXKrAF5GMlNTAN7M6M3vVzF4ys1XJbCtRFkXKeWlbEx3d0VSXIiKSUCNxhH+hu89197S9tm1ftZFyuqIxXt7WlOpSREQSSl06R1gwtRwzXRBFRDJPsgPfgUfNbLWZLTnaAma2xMxWmdmqhoaGJJdzYqVFucycOFYzZ4pIxkl24L/T3c8GLgc+Y2bnH7mAuy919wXuvqCqqirJ5QzMokg5q7fspzsaS3UpIiIJk9TAd/ft8b97gAeA2mS2lyi1kXLau6Ks3dGc6lJERBImaYFvZsVmVtJ7H3g38Fqy2kukhTW9FzbXvDoikjmSeYQ/AXjGzF4GVgIPufsfk9hewlSV5DOtqlgnbkUko+Qka8PuvgmYk6ztJ9uiSDkPvbKTWMwJhSzV5YiIDJuGZR5DbaSc5o4e3tjdkupSREQSQoF/DLWRCkDj8UUkcyjwj2FyWSGTywoV+CKSMRT4x7EoUs6Kzftw91SXIiIybAMKfDP7vJmNtcAdZrbGzN6d7OJSrTZSzt7WTjbvbUt1KSIiwzbQI/xPuHszwVj6ccCNwHeSVlWa6L2w+Qp164hIBhho4PeOS7wC+E93X9vnuYwVqSymcky++vFFJCMMNPBXm9mjBIH/SPwXtBk/0YyZsShSrsAXkYww0MD/H8BXgYXu3g7kAh9PWlVppDZSzvamg9Tvb091KSIiwzLQwD8HeMPdm8zsBuBrwIHklZU+evvxdZQvIqPdQAP/VqDdzOYAXwY2Ar9MWlVpZMaEEsYW5CjwRWTUG2jg93gwGP0DwI/c/cdASfLKSh+hkFGrfnwRyQADDfwWM7uZYDjmQ2YWIujHzwq1kXI27W1jT0tHqksRERmygQb+h4BOgvH4u4Bq4N+SVlWa6Z1X54XN+1NciYjI0A0o8OMhfxdQambvAzrcPSv68AFmTxpLUV5YF0QRkVFtoFMrXEdwEZO/AK4DVpjZB5NZWDrJDYeYP3WcfnErIqPaQLt0/pFgDP5fuftfElyb9uvJKyv91NaU88buFprau1JdiojIkAw08EPxC5H3ahzEuhmhNlKOO6yqUz++iIxOAw3tP5rZI2b2MTP7GPAQ8PBAVjSzsJm9aGYPDrXIdDDn5DLywiFW1qlbR0RGpwFd09bdbzKza4Fz408tdfcHBtjG54F1wNgh1Jc2CnLDzD25TP34IjJqDbhbxt3vc/cvxW8DCnszqwbeC9w+1ALTSW2knNe2H6CtsyfVpYiIDNpxA9/MWsys+Si3FjNrHsD2vw/8PRkys2ZtpJxozFmzVf34IjL6HDfw3b3E3cce5Vbi7sftoomP19/j7qtPsNwSM1tlZqsaGhqG8BZGztlTxxEOmaZZEJFRKZkjbc4FrjSzOuAe4CIz+9WRC7n7Undf4O4LqqqqkljO8I3Jz+GMSWPVjy8io1LSAt/db3b3anevAa4HnnD3G5LV3kipjZTz0rYmOrqjqS5FRGRQsmosfSLURiro6onxSn1WXA5ARDLIiAS+uz/p7u8bibaSbWHNOADNqyMio46O8AeprCiPmRNL1I8vIqOOAn8IaiPlrN6yn55oRow2FZEsocAfgtpIOe1dUdbuGMhPEURE0oMCfwhqa3RhcxEZfRT4QzB+bAGRymL144vIqKLAH6LamnJeqNtHLOapLkVEZEAU+ENUGynnwMFu3tzTkupSREQGRIE/RLUR9eOLyOiiwB+i6nGFTCotUD++iIwaCvwhMjNqI+Ws3LwPd/Xji0j6U+APQ22kgoaWTuoa21NdiojICSnwh+FwP77m1RGR9KfAH4ZTqoqpKM5TP76IjAoK/GHo248vIpLuFPjDVBspp37/QbY3HUx1KSIixzX6A98dHvwSbHg8Jc339uO/oKN8EUlzoz/wD+6Hrc/BXdfCH74C3SN7pD1z4lhKCnLUjy8iaW/0B35ROXxyGSz6NKz4KSy9EHa9NmLNh0PGwppyjdQRkbSXtMA3swIzW2lmL5vZWjP7VrLaIrcALv8O3HAfHNwHt10Iz/4IYiNzgZLaSDkbG9rY29o5Iu2JiAxFMo/wO4GL3H0OMBe4zMwWJ7E9mH4JfPo5mH4pPPqP8KuroXlHUpsE9eOLyOiQtMD3QGv8YW78lvw5CIor4Pq74P23wLaVcOs74PXfJbXJMyaVUpgbVj++iKS1pPbhm1nYzF4C9gCPufuKZLbXp2GY/zH41HIYVwO/uRH++zPQ2XqiNYckLyfE2VPLNB5fRNJaUgPf3aPuPheoBmrN7IwjlzGzJWa2ysxWNTQ0JLaAyunwPx6D8/4OXrwLfvpOqF+V2DbiamsqWLermQMHu5OyfRGR4RqRUTru3gQsAy47ymtL3X2Buy+oqqpKfOPhXLj46/DxhyEWhTveDU/9H4j2JLSZ2kg57vDjZW/R2RNN6LZFRBIhmaN0qsysLH6/ELgUWJ+s9k5o6jvg08/AGdfCsm/Dz6+A/XUJ2/zCmnG8f84klj69iff8+9M89WaCv62IiAxTMo/wTwKWmdkrwAsEffgPJrG9EysohWtvg2tuhz3r4NZ3wkt3B7/WHaaccIgffngev/xELWbGX925kr/5z9WackFE0oal08U7FixY4KtWJaeP/W2atsL9n4Ktz8Lsq+F9/w6F4xKy6c6eKLcv38wPn9iAYXz2oun89XkR8nPCCdm+iEgvM1vt7gsGsuzo/6XtUJVNgY89CBd/A9b9Hm49FzYvT8im83PCfObC6Tz+pQs4/7RK/u2RN7j8+8tZvkHdPCKSOtkb+AChMJz35WAkT24h/OL98Ng3oKcrIZuvHlfEz25cwH98fCFRd268YyWfuWsNOw+om0dERl72dukcqasNHvkHWP1zOGlO0M9fdVrCNt/RHeW2pzfxo2VvEQ4Zn7v4VD5xboS8nOze54rI8KhLZyjyioNf517/X3CgHn52Prxwe0JO6AIU5Ib524tP5fEvXcC50yv5zh/Wc/ktT/Pnt/YmZPsiIieiwD/SzPfCp58NhnE+9GW4+3poTVzf+8nlRdz2lwu482ML6I46H719BZ/9rzXsOtCRsDZERI5GXTrHEovByqVBn37BWPjAT+C0dye0iY7uKD99aiM/eXIjuSHj85ecysfPjZAb1n5YRAZGXTqJEArB4r+BJU/CmAnwX38BD/1dQi+wUpAb5guXnMbjX7yAxdMq+NeH13PFLct5dqO6eUQk8RT4JzLhdPjrP8E5n4UXboOfXQA7X0loE1MqirjjYwu5/S8XcLA7ykduW8Hn7n6R3c3q5hGRxFHgD0RuAbzn23DjA9BxAG67CP78g4RfYOWS0yfw+Jcu4HMXn8of1+7i4v/7FLcv30R3dGQu5CIimU19+IPVvg9+/7ngx1qR8+Gqn0Lp5IQ3U7e3jX/6/VqefKOBGRNK+OcPzGbRtIqEtyMio5v68JOpqByu+0+48kdQvzq4wMraBxLeTE1lMf/xsYUsvXE+rZ09fGjp83zx1y+xp0XdPCIyNDrCH47GjXD/J2H7apj7Ubj8f0N+ycDWdYdoF/R0QE9n/O8Rj6Od0NNJZ0c7f3p1K8+u305ROMolp5Uyf3IR4WjnoWX6rRfOg3k3QOSC4GIwIpKxBnOEr8Afrmh3ML/+8u/C2GqYMDse1gMI82FyDMspgJx8OPQ3H9obg9uks4OpI2ZcEYw6EpGMM5jAz0l2MRkvnAsX/SNMvxge+yY018fDtwAKyvoEcQHk5PV/HD7i8QCW8XAej29o4p//sIltzd1cc3Y1N18+i6qS/MM1dXfAy/8Ff74Ffv1RqJoJ7/xicC2AcG7qPisRSSkd4Y9S7V09/OiJt7ht+SYKcsN8+dLTuGHxVHL6/mgr2hOcX3jme7Dn9WCG0HM/D3NvCEYeiciopy6dLLKxoZV/+t1alm/Yy6yTxvIvV81m/tTy/gvFYrDhEVj+f6H+BSgeD+d8BhZ8IvgVsYiMWgr8LOPu/OG1XfzPB19n54EOrjl7Mh+cX83CmvL+0zS4Q90zQfBvWhZcAax2CSz6NBRryKfIaKTAz1JtnT388Im3uPOZzXRFY5Tk53DeaZVcOGM875oxvn8///Y1QVfPut9DbhGc/Vfwjs9CaXXq3oCIDJoCP8u1dfbwzFt7eWLdHpa9sYc9LZ2YwVnVZVw0YzwXzRzP7EljCYUMGt6AZ74Pr/waLARzrodzvwCV01P9NkRkANIi8M3sZOCXwATAgaXufsvx1lHgJ567s3ZHM0+s38MT6/fwcn0T7lBVks+FM6q4aOZ43nlqFWMO7oBnfwhrfhkMHZ19FbzzS3DSWal+CyJyHOkS+CcBJ7n7GjMrAVYDV7n768daR4GffHtbO3nqjQaeWL+Hp99soKWzh9ywsShSwYUzx3PJFGPqmz+HF+6AzmaYfmkwln/qOakuXUSOIi0C/20Nmf038CN3f+xYyyjwR1Z3NMaquv0seyM4+n9rTysAkcpiLp9eyId4hClv/hxrb4Qp74DzvgTTL9Gvd0XSSNoFvpnVAE8DZ7h78xGvLQGWAEyZMmX+li1bkl6PHN3WxnaeWL+bJ95o4PmNjXRFY1TmR/m7qhW8v/Veijt2wcQzg66e0z8QXAReRFIqrQLfzMYATwHfdvf7j7esjvDTR3tXD39+q5En1u9h2fo9NDa3clX4GT5f8DDV0Xo6x0bIveCLhOZ8OPh1sIikRNoEvpnlAg8Cj7j79060vAI/Pbk7r+9sZtn6PTy5bifjdzzOp8P/zZmhOppyqthx+ic5+ZJPUTK2LNWlimSdtAh8MzPgF8A+d//CQNZR4I8O+9q6eOqN3exY/RCLtv+cBaxjn5fw+Nhr6Jj3Cc49czrTKosx9fWLJF26BP47geXAq0DvJZv+wd0fPtY6CvzRpyca440XHiP/+VuY3vRnWryQu6KX8EjJNcyZNYNzTqlgcaSC0iJN2iaSDGkR+EOhwB/ldr1K2xPfpfDN3xElzP2x87mv+x2sYgYzJ5ZxzikVnDOtgtpp5Ywt0A5AJBEU+JJajRvhz7fgr/wG6zlIa14ly3PP5RcHzmZFzymYhThjcinnTKtg8SkVLKwpZ0y+ZuoWGQoFvqSHzlZ484/BFM0bHoNoJ51FE3mt9ELu61zIb3dPpDsK4ZBxVnWwAzjnlAoWTC2nME9DPkUGQoEv6aej+XD4v/U4RLuIja1mx+TLWJZzLg/sGs8r25vpiTm5YWNO9eEuoLOnjqMgVzsAkaNR4Et66zgA6x8Own/jExDrhrKpdM38AK+UXsRj+yfw/KZ9vLr9ADGHvJwQ804+vAOYO6WM/BztAERAgS+jycH9sP6hIPw3PQmxHiifBrOvpvXUK1nROpHnNu3juU2NvL6zGXcoyA0xf+q4Q11AZ1WX9Z/3XySLKPBldGrfF8zPv/Z+2Pw0eAwqToXZV8MZ13BgzHRWbG7kuU2NPLexkfW7WgAoyguzoKacxdPKOWdaBWdOLu1/qUeRDKbAl9GvbS+s+x28dj9s+XMQ/lUzYfY1wQ6g6jT2tXWxYtPhHcCG+ORvY/JzWFgzLt4FVMmpE8YQDhlhM8zQD8IkoyjwJbO07A7Cf+0DsOVZwGH8bDjj6mAHUHEKAA0tnTwf3wE8v6mRTQ1tR92cGYTs8A4gZEbIIBSyw/fN4o97XzdCocP3zSDc537IjHB8eYtvIxyyQ/dDZlSV5DOtcgzTqoqZVlVMpLKYojwNR5XhUeBL5mreefjIf9vzwXMTzzx85F8eObTo7uYOnt/USP3+g7g7MYeYO7HY4ftRd9zp91zvLRojvl78tdjh+8F6TizWdx2OsmywTNSd3c0dbG86SN9/cieVFgQ7gEM7gjFMqyxmcllhcEUykRNQ4Et2OLAdXv9/wZF//QvBc5PmBcF/+lUwbmpq6zuKju4odY1tbGpoY1NDK5sa2ti4N7jf0tFzaLn8nBCRyuK37wyqivUrZelHgS/Zp2krrI2H/441wXOTFwThP/tqKJ2c2vpOwN3Z29oV7AT2Ht4ZbNrbxtZ97URjh/+dVo7JZ1rw6aeqAAAK/UlEQVRVMaccsTM4eVyhTlZnIQW+ZLf9dUHwr30Adr4MGETOhzkfhlnvh/wxqa5wULp6Ymzd137UncG+tq5Dy+WGjSnlRYe6hfp2EZUX5+lkdYZS4Iv0atwIr/4WXr472BHkFgWhP+d6iFww6q/a1dTexcbe7qE+O4Mtje10RWOHlistzD10orgwN3zo3EL/cw9HnucIHrs70T7nONyJP46f/4ifC+ld9kTnP2LuhMyoqSxm5sQSZk4sYcbEEqaPH6Mf1A2BAl/kSO6wbQW8fE8wzr/jAJScBGf+RXDkP+H0VFeYUNGYU7+/PThH0GdnsKWxne5oDIuPUjo0qih09FFHdsxRS0cfvdT7vJkRPnJUU+jw/Z6os7GhlQ27Ww/tmMIhY1plMTNPGhvsBCaUMPOkEiaXFerbyXEo8EWOp7sjmNfn5XvgrceCX/dOPCsI/jM/CGPGp7rCrNETjVHX2Ma6nS28sauF9btaWL+rmfr9Bw8tU5Kfw4z4t4BDO4OJJTp5HafAFxmotr3w2n1Bl8+OF8HCMP0SmPMhmHEF5BamusKs1NLRzZu74zuA+M5g3a7mfiOZJpcVBjuB+A5g1kljiVQWZ900Gwp8kaHYsx5euQde+Q00b4f8sTD7quDI/+TFEMquIEk37s7OAx2s39Xcb0ewsaGVnvgoprxwiFPGj+l3bmDWSWMZX5Kfsd1CCnyR4YhFoW45vPxreP2/obsNyqbCWR8KTvbGf9kr6aGrJ8bGhtZD3wLeiO8MdjV3HFqmrCg3vhMYe+hbwWkTSihO0IV3vM8J7N4T09D/scfA6X+SHIeYB7/+njC2YEhtp0Xgm9mdwPuAPe5+xkDWUeBL2ulqg3UPBl0+m54EHKprg+CffTUUlae6woHrPgjNO4KRSsVVEM7saR2a2rtYv6v33EDwreDNXS20dUUPLTNhbD4hs35h7e44h0crxXO530imIwN+uCrH5LPqa5cMad10CfzzgVbglwp8yQjNO4LunpfvgYZ1EM6D094TdPlMvxRy8lJbX7QHmuth/xZo2vL2v627Dy9roSD0x0yAkonBbcxEKJkQ/xu/FY9P/ftKoFjMqd9/8NAOYNu+doBDo43MDOPwSCTrN3Lp8Aiktz2m/5xMwBGjlug/4smAPo8Lc8NcNW9oPw5Mi8CPF1IDPKjAl4ziDrteCYL/1d9CWwMUlgcjfOZcD5PODr6jJ6Pd1t1HBHnd4ccHtoMfPnrFQjC2Ophiomxq8Le0GrrbgwnpWncFf1t2BtttawhmJT1SUUX/nUDvTmLMhGBoa+9OIndoXRIyPKMq8M1sCbAEYMqUKfO3bNmStHpEEi7aHVy16+V7ggu5RDuh8rSgv/+sD0HZyYPb3sH9xz5Cb9oKPR39ly8e3z/Q+/4trYbwIIYuRnugfW+wA+i7Q2jdBS3xW+vu4Bbrefv6BaXBDuBYO4Te54byS2f34NxKrCfYqcV6gsceO3z/0GvxW9/l3va4J1jXQlA2Jbjl5A++rjQwqgK/Lx3hy6h2sCk4yfvyPbD1WcCg5p1Bl8/pV0J+CXS1B8HdL9Dr4n+3QueB/tssKD0izGv6HK2fDHlFI/8+YzFobzxih3CMnUS06+3r55VAcWXwLWgg4ezRo3/zSCiDsZP6fL41fW5Tgx1Vmo7yUeCLpNq+zUF//yv3wL5NkFMYHNm2NfRfLqcwOLo81lF6YVlq6k8E9+AbS+vuPt8Q4juEtoYgQC0cTG8RCsfv58Qf5wRH34fu974WOuJxOL5czhHLHm2bRzy2cLBDOrAt2Okeum2Blh3930tOYfDfpHcn0G+nMBXyikf4wz1sMIGf2afpRVKlPALv+gpc8PfB1M2v/jYYJTNuKpTVHA70MePT9shx2MyCUUxF5TB+VqqrGZzujuCbWO9OoPeb2P46qHsGulr7L19c1f9bQd8dwthJaTNnUzJH6dwNvAuoBHYD33T3O463jo7wRSTtuQfXX95fB/s3v32HcKC+fxdUKDc4l9Ovm6jm8E5hmN/i0uII390/nKxti4ikjBkUVwS36vlvfz3aHYR+326i3h3CjheDbq6+CsqCb0Cf+GPSS1eXjohIIoVzgy69Ppfb7Odg0+GT9r07hKONekoCBb6IyEgqLAtuJ80Z8aY1G5SISJZQ4IuIZAkFvohIllDgi4hkCQW+iEiWUOCLiGQJBb6ISJZQ4IuIZIm0uqatmTUAQ50QvxLYm8ByRjN9Fv3p8+hPn8dhmfBZTHX3qoEsmFaBPxxmtmqgEwhlOn0W/enz6E+fx2HZ9lmoS0dEJEso8EVEskQmBf7SVBeQRvRZ9KfPoz99Hodl1WeRMX34IiJyfJl0hC8iIsehwBcRyRKjPvDN7DIze8PM3jKzr6a6nlQys5PNbJmZvW5ma83s86muKdXMLGxmL5rZg6muJdXMrMzM7jWz9Wa2zszOSXVNqWRmX4z/O3nNzO42s4JU15RsozrwzSwM/Bi4HDgd+LCZnZ7aqlKqB/iyu58OLAY+k+WfB8DngXWpLiJN3AL80d1nAnPI4s/FzCYDnwMWuPsZQBi4PrVVJd+oDnygFnjL3Te5exdwD/CBFNeUMu6+093XxO+3EPyDnpzaqlLHzKqB9wK3p7qWVDOzUuB84A4Ad+9y96bUVpVyOUChmeUARcCOFNeTdKM98CcD2/o8rieLA64vM6sB5gErUltJSn0f+HsglupC0kAEaAD+I97FdbuZFae6qFRx9+3Ad4GtwE7ggLs/mtqqkm+0B74chZmNAe4DvuDuzamuJxXM7H3AHndfnepa0kQOcDZwq7vPA9qArD3nZWbjCHoDIsAkoNjMbkhtVck32gN/O3Byn8fV8eeylpnlEoT9Xe5+f6rrSaFzgSvNrI6gq+8iM/tVaktKqXqg3t17v/HdS7ADyFaXAJvdvcHdu4H7gXekuKakG+2B/wJwqplFzCyP4KTL71JcU8qYmRH00a5z9++lup5Ucveb3b3a3WsI/r94wt0z/gjuWNx9F7DNzGbEn7oYeD2FJaXaVmCxmRXF/91cTBacxM5JdQHD4e49ZvZZ4BGCs+x3uvvaFJeVSucCNwKvmtlL8ef+wd0fTmFNkj7+FrgrfnC0Cfh4iutJGXdfYWb3AmsIRre9SBZMs6CpFUREssRo79IREZEBUuCLiGQJBb6ISJZQ4IuIZAkFvohIllDgiySAmb1LM3JKulPgi4hkCQW+ZBUzu8HMVprZS2b2s/h8+a1m9u/xudH/ZGZV8WXnmtnzZvaKmT0Qn38FM5tuZo+b2ctmtsbMTolvfkyf+ebviv+CUyRtKPAla5jZLOBDwLnuPheIAh8FioFV7j4beAr4ZnyVXwJfcfezgFf7PH8X8GN3n0Mw/8rO+PPzgC8QXJthGsEvn0XSxqieWkFkkC4G5gMvxA++C4E9BNMn/zq+zK+A++Pzx5e5+1Px538B/NbMSoDJ7v4AgLt3AMS3t9Ld6+OPXwJqgGeS/7ZEBkaBL9nEgF+4+839njT7+hHLDXW+kc4+96Po35ekGXXpSDb5E/BBMxsPYGblZjaV4N/BB+PLfAR4xt0PAPvN7Lz48zcCT8WvJFZvZlfFt5FvZkUj+i5EhkhHIJI13P11M/sa8KiZhYBu4DMEFwOpjb+2h6CfH+CvgJ/GA73v7JI3Aj8zs3+Ob+MvRvBtiAyZZsuUrGdmre4+JtV1iCSbunRERLKEjvBFRLKEjvBFRLKEAl9EJEso8EVEsoQCX0QkSyjwRUSyxP8H9mZZW+Yo19oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = hist\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "'''\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "'''\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "82/82 [==============================] - 1044s 13s/step - loss: 0.8930 - val_loss: 0.7626\n",
      "Epoch 2/10\n",
      "82/82 [==============================] - 1123s 14s/step - loss: 1.0897 - val_loss: 1.0334\n",
      "Epoch 3/10\n",
      "82/82 [==============================] - 1184s 14s/step - loss: 1.1389 - val_loss: 0.8034\n",
      "Epoch 4/10\n",
      "82/82 [==============================] - 1059s 13s/step - loss: 0.9278 - val_loss: 0.9471\n",
      "Epoch 5/10\n",
      "82/82 [==============================] - 1087s 13s/step - loss: 1.0433 - val_loss: 1.0065\n",
      "Epoch 6/10\n",
      "82/82 [==============================] - 1056s 13s/step - loss: 0.7671 - val_loss: 0.9030\n",
      "Epoch 7/10\n",
      "82/82 [==============================] - 1056s 13s/step - loss: 0.9072 - val_loss: 1.2553\n",
      "Epoch 8/10\n",
      "82/82 [==============================] - 1056s 13s/step - loss: 0.8690 - val_loss: 0.8512\n",
      "Epoch 9/10\n",
      "82/82 [==============================] - 1056s 13s/step - loss: 0.9312 - val_loss: 1.0551\n",
      "Epoch 10/10\n",
      "82/82 [==============================] - 1056s 13s/step - loss: 0.9320 - val_loss: 1.0535\n"
     ]
    }
   ],
   "source": [
    "hist = train_model.fit_generator(train_triplet_batch, \n",
    "                          steps_per_epoch=train_steps,\n",
    "                          epochs=10,\n",
    "                          validation_data=train_triplet_batch,\n",
    "                          validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'loss'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = hist\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "'''\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "'''\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('triplet_model_2.h5')\n",
    "train_model.save('triplet_train_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedded = np.zeros((len(img), 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(img)):\n",
    "    tr_img = (img[i] / 255.).astype(np.float32)\n",
    "    # obtain embedding vector for image\n",
    "    embedded[i] = model.predict(np.expand_dims(tr_img, axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(emb1, emb2):\n",
    "    return np.sum(np.square(emb1 - emb2))\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    return img[...,::-1]\n",
    "def show_pair(idx1, idx2):\n",
    "    plt.figure(figsize=(8,3))\n",
    "    plt.suptitle(f'Distance = {distance(embedded[idx1], embedded[idx2]):.2f}')\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(load_image(metadata[idx1].image_path()))\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(load_image(metadata[idx2].image_path()));    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1701521198528336\n",
      "None\n",
      "0.8277562602678239\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def emb(x, y):\n",
    "    print(distance(embedded[x], embedded[y]))\n",
    "print(emb(2, 3))\n",
    "print(emb(2, 327))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1024,)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = model.predict(np.expand_dims(X_test[0], axis=0))[0]\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 5, 7, 9]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'accuracy_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-0a83b57a35d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0macc_knn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0macc_svc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accuracy_score' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Numerical encoding of identities\n",
    "y = np.array(label)\n",
    "\n",
    "train_idx = [x for x in range(len(img)) if x % 2 != 0]\n",
    "test_idx = [x for x in range(len(img)) if x % 2 == 0]\n",
    "print(train_idx[:5])\n",
    "\n",
    "# 50 train examples of 10 identities (5 examples each)\n",
    "X_train = embedded[train_idx]\n",
    "# 50 test examples of 10 identities (5 examples each)\n",
    "X_test = embedded[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "svc = LinearSVC()\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "acc_knn = accuracy_score(y_test, knn.predict(X_test))\n",
    "acc_svc = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(f'KNN accuracy = {acc_knn}, SVM accuracy = {acc_svc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras._impl.keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fc6', 'fc7', 'fc8', 'conv3', 'conv2', 'conv1', 'conv5', 'conv4'])\n"
     ]
    }
   ],
   "source": [
    "net_data = np.load(open(\"bvlc_alexnet.npy\", \"rb\"), encoding=\"latin1\").item()\n",
    "print(net_data.keys())\n",
    "conv1W = tf.Variable(net_data[\"conv1\"][0])\n",
    "conv1b = tf.Variable(net_data[\"conv1\"][1])\n",
    "conv2W = tf.Variable(net_data[\"conv2\"][0])\n",
    "conv2b = tf.Variable(net_data[\"conv2\"][1])\n",
    "conv3W = tf.Variable(net_data[\"conv3\"][0])\n",
    "conv3b = tf.Variable(net_data[\"conv3\"][1])\n",
    "conv4W = tf.Variable(net_data[\"conv4\"][0])\n",
    "conv4b = tf.Variable(net_data[\"conv4\"][1])\n",
    "conv5W = tf.Variable(net_data[\"conv5\"][0])\n",
    "conv5b = tf.Variable(net_data[\"conv5\"][1])\n",
    "fc6W = tf.Variable(net_data[\"fc6\"][0])\n",
    "fc6b = tf.Variable(net_data[\"fc6\"][1])\n",
    "fc7W = tf.Variable(net_data[\"fc7\"][0])\n",
    "fc7b = tf.Variable(net_data[\"fc7\"][1])\n",
    "fc8W = tf.Variable(net_data[\"fc8\"][0])\n",
    "fc8b = tf.Variable(net_data[\"fc8\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.engine.topology import Layer\n",
    "import keras.backend as K\n",
    "\n",
    "\n",
    "class SpatialPyramidPooling(Layer):\n",
    "    \"\"\"Spatial pyramid pooling layer for 2D inputs.\n",
    "    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n",
    "    K. He, X. Zhang, S. Ren, J. Sun\n",
    "    # Arguments\n",
    "        pool_list: list of int\n",
    "            List of pooling regions to use. The length of the list is the number of pooling regions,\n",
    "            each int in the list is the number of regions in that pool. For example [1,2,4] would be 3\n",
    "            regions with 1, 2x2 and 4x4 max pools, so 21 outputs per feature map\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "    # Output shape\n",
    "        2D tensor with shape:\n",
    "        `(samples, channels * sum([i * i for i in pool_list])`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pool_list, **kwargs):\n",
    "\n",
    "        self.dim_ordering = K.image_dim_ordering()\n",
    "        assert self.dim_ordering in {'tf', 'th'}, 'dim_ordering must be in {tf, th}'\n",
    "\n",
    "        self.pool_list = pool_list\n",
    "\n",
    "        self.num_outputs_per_channel = sum([i * i for i in pool_list])\n",
    "\n",
    "        super(SpatialPyramidPooling, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if self.dim_ordering == 'th':\n",
    "            self.nb_channels = input_shape[1]\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            self.nb_channels = input_shape[3]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.nb_channels * self.num_outputs_per_channel)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'pool_list': self.pool_list}\n",
    "        base_config = super(SpatialPyramidPooling, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        input_shape = K.shape(x)\n",
    "\n",
    "        if self.dim_ordering == 'th':\n",
    "            num_rows = input_shape[2]\n",
    "            num_cols = input_shape[3]\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            num_rows = input_shape[1]\n",
    "            num_cols = input_shape[2]\n",
    "\n",
    "        row_length = [K.cast(num_rows, 'float32') / i for i in self.pool_list]\n",
    "        col_length = [K.cast(num_cols, 'float32') / i for i in self.pool_list]\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        if self.dim_ordering == 'th':\n",
    "            for pool_num, num_pool_regions in enumerate(self.pool_list):\n",
    "                for jy in range(num_pool_regions):\n",
    "                    for ix in range(num_pool_regions):\n",
    "                        x1 = ix * col_length[pool_num]\n",
    "                        x2 = ix * col_length[pool_num] + col_length[pool_num]\n",
    "                        y1 = jy * row_length[pool_num]\n",
    "                        y2 = jy * row_length[pool_num] + row_length[pool_num]\n",
    "\n",
    "                        x1 = K.cast(K.round(x1), 'int32')\n",
    "                        x2 = K.cast(K.round(x2), 'int32')\n",
    "                        y1 = K.cast(K.round(y1), 'int32')\n",
    "                        y2 = K.cast(K.round(y2), 'int32')\n",
    "                        new_shape = [input_shape[0], input_shape[1],\n",
    "                                     y2 - y1, x2 - x1]\n",
    "                        x_crop = x[:, :, y1:y2, x1:x2]\n",
    "                        xm = K.reshape(x_crop, new_shape)\n",
    "                        pooled_val = K.max(xm, axis=(2, 3))\n",
    "                        outputs.append(pooled_val)\n",
    "\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            for pool_num, num_pool_regions in enumerate(self.pool_list):\n",
    "                for jy in range(num_pool_regions):\n",
    "                    for ix in range(num_pool_regions):\n",
    "                        x1 = ix * col_length[pool_num]\n",
    "                        x2 = ix * col_length[pool_num] + col_length[pool_num]\n",
    "                        y1 = jy * row_length[pool_num]\n",
    "                        y2 = jy * row_length[pool_num] + row_length[pool_num]\n",
    "\n",
    "                        x1 = K.cast(K.round(x1), 'int32')\n",
    "                        x2 = K.cast(K.round(x2), 'int32')\n",
    "                        y1 = K.cast(K.round(y1), 'int32')\n",
    "                        y2 = K.cast(K.round(y2), 'int32')\n",
    "\n",
    "                        new_shape = [input_shape[0], y2 - y1,\n",
    "                                     x2 - x1, input_shape[3]]\n",
    "\n",
    "                        x_crop = x[:, y1:y2, x1:x2, :]\n",
    "                        xm = K.reshape(x_crop, new_shape)\n",
    "                        pooled_val = K.max(xm, axis=(1, 2))\n",
    "                        outputs.append(pooled_val)\n",
    "\n",
    "        if self.dim_ordering == 'th':\n",
    "            outputs = K.concatenate(outputs)\n",
    "        elif self.dim_ordering == 'tf':\n",
    "            #outputs = K.concatenate(outputs,axis = 1)\n",
    "            outputs = K.concatenate(outputs)\n",
    "            #outputs = K.reshape(outputs,(len(self.pool_list),self.num_outputs_per_channel,input_shape[0],input_shape[1]))\n",
    "            #outputs = K.permute_dimensions(outputs,(3,1,0,2))\n",
    "            #outputs = K.reshape(outputs,(input_shape[0], self.num_outputs_per_channel * self.nb_channels))\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Conv2D, Activation, MaxPooling2D, Dense\n",
    "from keras.engine.topology import Input\n",
    "from keras.models import Model\n",
    "\n",
    "batch_size = 32\n",
    "num_channels = 3\n",
    "num_classes = 10\n",
    "img_x, img_y, img_z = img[0].shape\n",
    "\n",
    "img_input = Input(batch_shape=(batch_size, None, None, num_channels))\n",
    "x = Conv2D(96, (11, 11), strides=(4, 4), activation='relu', padding='same', name='conv1')(img_input)\n",
    "x = local_resp_norm(name='lrn_1')(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same', name='conv3')(x)\n",
    "x = Conv2D(32, (3, 3), activation='relu', name='conv4')(x)\n",
    "x = SpatialPyramidPooling([1, 2, 4])(x)\n",
    "predictions = Dense(nb_classes, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=img_input, outputs=predictions)\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "32/32 [==============================] - 38s 1s/step - loss: 8.6585 - acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1b6383f97b8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.fit_generator()\n",
    "\n",
    "# train on 64x64x3 images\n",
    "model.fit(s[0][:32], s[1][:32])\n",
    "# train on 32x32x3 images\n",
    "#model.fit(np.random.rand(batch_size, num_channels, 32, 32), np.zeros((batch_size, num_classes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = to_categorical(y_train, nb_classes)\n",
    "Y_test = to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import *\n",
    "from tensorflow.python.keras.models import *\n",
    "#from keras.engine.topology import Layer\n",
    "#import keras.backend as K\n",
    "\n",
    "\n",
    "class SpatialPyramidPooling(Layer):\n",
    "    \"\"\"Spatial pyramid pooling layer for 2D inputs.\n",
    "    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n",
    "    K. He, X. Zhang, S. Ren, J. Sun\n",
    "    # Arguments\n",
    "        pool_list: list of int\n",
    "            List of pooling regions to use. The length of the list is the number of pooling regions,\n",
    "            each int in the list is the number of regions in that pool. For example [1,2,4] would be 3\n",
    "            regions with 1, 2x2 and 4x4 max pools, so 21 outputs per feature map\n",
    "    # Input shape\n",
    "        4D tensor with shape:\n",
    "        `(samples, channels, rows, cols)` if dim_ordering='th'\n",
    "        or 4D tensor with shape:\n",
    "        `(samples, rows, cols, channels)` if dim_ordering='tf'.\n",
    "    # Output shape\n",
    "        2D tensor with shape:\n",
    "        `(samples, channels * sum([i * i for i in pool_list])`\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pool_list, **kwargs):\n",
    "        \n",
    "        self.pool_list = pool_list\n",
    "\n",
    "        self.num_outputs_per_channel = sum([i * i for i in pool_list])\n",
    "\n",
    "        super(SpatialPyramidPooling, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.nb_channels = input_shape[3]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.nb_channels * self.num_outputs_per_channel)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'pool_list': self.pool_list}\n",
    "        base_config = super(SpatialPyramidPooling, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        input_shape = tf.shape(x)\n",
    "\n",
    "        num_rows = input_shape[1]\n",
    "        num_cols = input_shape[2]\n",
    "\n",
    "        row_length = [tf.cast(num_rows, 'float32') / i for i in self.pool_list]\n",
    "        col_length = [ttf.cast(num_cols, 'float32') / i for i in self.pool_list]\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        for pool_num, num_pool_regions in enumerate(self.pool_list):\n",
    "            for jy in range(num_pool_regions):\n",
    "                for ix in range(num_pool_regions):\n",
    "                    x1 = ix * col_length[pool_num]\n",
    "                    x2 = ix * col_length[pool_num] + col_length[pool_num]\n",
    "                    y1 = jy * row_length[pool_num]\n",
    "                    y2 = jy * row_length[pool_num] + row_length[pool_num]\n",
    "\n",
    "                    x1 = tf.cast(tf.round(x1), 'int32')\n",
    "                    x2 = tf.cast(tf.round(x2), 'int32')\n",
    "                    y1 = tf.cast(tf.round(y1), 'int32')\n",
    "                    y2 = tf.cast(tf.round(y2), 'int32')\n",
    "\n",
    "                    new_shape = [input_shape[0], y2 - y1,\n",
    "                                 x2 - x1, input_shape[3]]\n",
    "\n",
    "                    x_crop = x[:, y1:y2, x1:x2, :]\n",
    "                    xm = tf.reshape(x_crop, new_shape)\n",
    "                    pooled_val = tf.max(xm, axis=(1, 2))\n",
    "                    outputs.append(pooled_val)\n",
    "\n",
    "        return tf.concatenate(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['fc6', 'fc7', 'fc8', 'conv3', 'conv2', 'conv1', 'conv5', 'conv4'])\n"
     ]
    }
   ],
   "source": [
    "net_data = np.load(open(\"bvlc_alexnet.npy\", \"rb\"), encoding=\"latin1\").item()\n",
    "print(net_data.keys())\n",
    "conv1W = tf.Variable(net_data[\"conv1\"][0])\n",
    "conv1b = tf.Variable(net_data[\"conv1\"][1])\n",
    "conv2W = tf.Variable(net_data[\"conv2\"][0])\n",
    "conv2b = tf.Variable(net_data[\"conv2\"][1])\n",
    "conv3W = tf.Variable(net_data[\"conv3\"][0])\n",
    "conv3b = tf.Variable(net_data[\"conv3\"][1])\n",
    "conv4W = tf.Variable(net_data[\"conv4\"][0])\n",
    "conv4b = tf.Variable(net_data[\"conv4\"][1])\n",
    "conv5W = tf.Variable(net_data[\"conv5\"][0])\n",
    "conv5b = tf.Variable(net_data[\"conv5\"][1])\n",
    "fc6W = tf.Variable(net_data[\"fc6\"][0])\n",
    "fc6b = tf.Variable(net_data[\"fc6\"][1])\n",
    "fc7W = tf.Variable(net_data[\"fc7\"][0])\n",
    "fc7b = tf.Variable(net_data[\"fc7\"][1])\n",
    "fc8W = tf.Variable(net_data[\"fc8\"][0])\n",
    "fc8b = tf.Variable(net_data[\"fc8\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(227, 227, 3)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros((1, 227,227,3)).astype(float32).shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "random_sample() takes at most 1 positional argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-7431f2460a82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.random_sample (numpy\\random\\mtrand\\mtrand.c:15497)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: random_sample() takes at most 1 positional argument (2 given)"
     ]
    }
   ],
   "source": [
    "random.sample(range(30),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_gen(batch_size=32):\n",
    "    maxlen = Y_train.shape[0]\n",
    "    random.sample(, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#Michael Guerzhoy and Davi Frossard, 2016\n",
    "#AlexNet implementation in TensorFlow, with weights\n",
    "#Details: \n",
    "#http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\n",
    "#\n",
    "#With code from https://github.com/ethereon/caffe-tensorflow\n",
    "#Model from  https://github.com/BVLC/caffe/tree/master/models/bvlc_alexnet\n",
    "#Weights from Caffe converted using https://github.com/ethereon/caffe-tensorflow\n",
    "#\n",
    "#\n",
    "################################################################################\n",
    "\n",
    "from numpy import *\n",
    "import os\n",
    "#from pylab import *\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#import matplotlib.cbook as cbook\n",
    "import time\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "import matplotlib.image as mpimg\n",
    "from scipy.ndimage import filters\n",
    "import urllib\n",
    "from numpy import random\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#from caffe_classes import class_names\n",
    "\n",
    "train_x = zeros((1, 227,227,3)).astype(float32)\n",
    "train_y = zeros((1, 1000))\n",
    "xdim = train_x.shape[1:]\n",
    "ydim = train_y.shape[1]\n",
    "batch_size = 64\n",
    "\n",
    "'''\n",
    "\n",
    "################################################################################\n",
    "#Read Image, and change to BGR\n",
    "\n",
    "\n",
    "im1 = (imread(\"laska.png\")[:,:,:3]).astype(float32)\n",
    "im1 = im1 - mean(im1)\n",
    "im1[:, :, 0], im1[:, :, 2] = im1[:, :, 2], im1[:, :, 0]\n",
    "\n",
    "im2 = (imread(\"poodle.png\")[:,:,:3]).astype(float32)\n",
    "im2[:, :, 0], im2[:, :, 2] = im2[:, :, 2], im2[:, :, 0]\n",
    "\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# (self.feed('data')\n",
    "#         .conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "#         .lrn(2, 2e-05, 0.75, name='norm1')\n",
    "#         .max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "#         .conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
    "#         .lrn(2, 2e-05, 0.75, name='norm2')\n",
    "#         .max_pool(3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "#         .conv(3, 3, 384, 1, 1, name='conv3')\n",
    "#         .conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
    "#         .conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
    "#         .fc(4096, name='fc6')\n",
    "#         .fc(4096, name='fc7')\n",
    "#         .fc(1000, relu=False, name='fc8')\n",
    "#         .softmax(name='prob'))\n",
    "'''\n",
    "\n",
    "def conv(input, kernel, biases, k_h, k_w, c_o, s_h, s_w,  padding=\"VALID\", group=1):\n",
    "    '''From https://github.com/ethereon/caffe-tensorflow\n",
    "    '''\n",
    "    c_i = input.get_shape()[-1]\n",
    "    assert c_i%group==0\n",
    "    assert c_o%group==0\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k, [1, s_h, s_w, 1], padding=padding)\n",
    "    \n",
    "    \n",
    "    if group==1:\n",
    "        conv = convolve(input, kernel)\n",
    "    else:\n",
    "        input_groups =  tf.split(input, group, 3)   #tf.split(3, group, input)\n",
    "        kernel_groups = tf.split(kernel, group, 3)  #tf.split(3, group, kernel) \n",
    "        output_groups = [convolve(i, k) for i,k in zip(input_groups, kernel_groups)]\n",
    "        conv = tf.concat(output_groups, 3)          #tf.concat(3, output_groups)\n",
    "    return  tf.reshape(tf.nn.bias_add(conv, biases), [-1]+conv.get_shape().as_list()[1:])\n",
    "\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None,) + xdim)\n",
    "\n",
    "\n",
    "#conv1\n",
    "#conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "k_h = 11; k_w = 11; c_o = 96; s_h = 4; s_w = 4\n",
    "\n",
    "conv1_in = conv(x, conv1W, conv1b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=1)\n",
    "conv1 = tf.nn.relu(conv1_in)\n",
    "\n",
    "#lrn1\n",
    "#lrn(2, 2e-05, 0.75, name='norm1')\n",
    "radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "lrn1 = tf.nn.local_response_normalization(conv1,\n",
    "                                                  depth_radius=radius,\n",
    "                                                  alpha=alpha,\n",
    "                                                  beta=beta,\n",
    "                                                  bias=bias)\n",
    "\n",
    "#maxpool1\n",
    "#max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "\n",
    "#conv2\n",
    "#conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
    "k_h = 5; k_w = 5; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "\n",
    "\n",
    "conv2_in = conv(maxpool1, conv2W, conv2b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv2 = tf.nn.relu(conv2_in)\n",
    "\n",
    "\n",
    "#lrn2\n",
    "#lrn(2, 2e-05, 0.75, name='norm2')\n",
    "radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "lrn2 = tf.nn.local_response_normalization(conv2,\n",
    "                                                  depth_radius=radius,\n",
    "                                                  alpha=alpha,\n",
    "                                                  beta=beta,\n",
    "                                                  bias=bias)\n",
    "\n",
    "#maxpool2\n",
    "#max_pool(3, 3, 2, 2, padding='VALID', name='pool2')                                                  \n",
    "k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "#conv3\n",
    "#conv(3, 3, 384, 1, 1, name='conv3')\n",
    "k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 1\n",
    "\n",
    "\n",
    "conv3_in = conv(maxpool2, conv3W, conv3b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv3 = tf.nn.relu(conv3_in)\n",
    "\n",
    "#conv4\n",
    "#conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
    "k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 2\n",
    "\n",
    "\n",
    "conv4_in = conv(conv3, conv4W, conv4b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv4 = tf.nn.relu(conv4_in)\n",
    "\n",
    "\n",
    "#conv5\n",
    "#conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
    "k_h = 3; k_w = 3; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "\n",
    "\n",
    "conv5_in = conv(conv4, conv5W, conv5b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "conv5 = tf.nn.relu(conv5_in)\n",
    "\n",
    "#maxpool5\n",
    "#max_pool(3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "maxpool5 = tf.nn.max_pool(conv5, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "#fc6\n",
    "#fc(4096, name='fc\n",
    "fc6 = tf.nn.relu_layer(tf.reshape(maxpool5, [-1, int(prod(maxpool5.get_shape()[1:]))]), fc6W, fc6b)\n",
    "\n",
    "#fc7\n",
    "#fc(4096, name='fc\n",
    "fc7 = tf.nn.relu_layer(fc6, fc7W, fc7b)\n",
    "\n",
    "#fc8\n",
    "#fc(1000, relu=False, name='fc\n",
    "fc8 = tf.nn.xw_plus_b(fc7, fc8W, fc8b)\n",
    "\n",
    "\n",
    "#prob\n",
    "#softmax(name='prob'))\n",
    "prob = tf.nn.softmax(fc8)\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "xdim = X_train[0].shape[1:]\n",
    "images_placeholder = tf.placeholder(tf.float32, (None,) + xdim)\n",
    "labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    output = sess.run(prob, feed_dict = {x:[im1,im2]})\n",
    "\n",
    "#Output:\n",
    "\n",
    "\n",
    "for input_im_ind in range(output.shape[0]):\n",
    "    inds = argsort(output)[input_im_ind,:]\n",
    "    print(\"Image\", input_im_ind)\n",
    "    '''\n",
    "    for i in range(5):\n",
    "        print(class_names[inds[-1-i]], output[input_im_ind, inds[-1-i]])\n",
    "    '''\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"This is an TensorFLow implementation of AlexNet by Alex Krizhevsky at all.\n",
    "\n",
    "Paper:\n",
    "(http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)\n",
    "\n",
    "Explanation can be found in my blog post:\n",
    "https://kratzert.github.io/2017/02/24/finetuning-alexnet-with-tensorflow.html\n",
    "\n",
    "This script enables finetuning AlexNet on any given Dataset with any number of\n",
    "classes. The structure of this script is strongly inspired by the fast.ai\n",
    "Deep Learning class by Jeremy Howard and Rachel Thomas, especially their vgg16\n",
    "finetuning script:\n",
    "Link:\n",
    "- https://github.com/fastai/courses/blob/master/deeplearning1/nbs/vgg16.py\n",
    "\n",
    "\n",
    "The pretrained weights can be downloaded here and should be placed in the same\n",
    "folder as this file:\n",
    "- http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\n",
    "\n",
    "@author: Frederik Kratzert (contact: f.kratzert(at)gmail.com)\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AlexNet(object):\n",
    "    \"\"\"Implementation of the AlexNet.\"\"\"\n",
    "\n",
    "    def __init__(self, x, keep_prob, num_classes, skip_layer,\n",
    "                 weights_path='DEFAULT'):\n",
    "        \"\"\"Create the graph of the AlexNet model.\n",
    "\n",
    "        Args:\n",
    "            x: Placeholder for the input tensor.\n",
    "            keep_prob: Dropout probability.\n",
    "            num_classes: Number of classes in the dataset.\n",
    "            skip_layer: List of names of the layer, that get trained from\n",
    "                scratch\n",
    "            weights_path: Complete path to the pretrained weight file, if it\n",
    "                isn't in the same folder as this code\n",
    "        \"\"\"\n",
    "        # Parse input arguments into class variables\n",
    "        self.X = x\n",
    "        self.NUM_CLASSES = num_classes\n",
    "        self.KEEP_PROB = keep_prob\n",
    "        self.SKIP_LAYER = skip_layer\n",
    "\n",
    "        if weights_path == 'DEFAULT':\n",
    "            self.WEIGHTS_PATH = 'bvlc_alexnet.npy'\n",
    "        else:\n",
    "            self.WEIGHTS_PATH = weights_path\n",
    "\n",
    "        # Call the create function to build the computational graph of AlexNet\n",
    "        self.create()\n",
    "\n",
    "    def create(self):\n",
    "        \"\"\"Create the network graph.\"\"\"\n",
    "        # 1st Layer: Conv (w ReLu) -> Lrn -> Pool\n",
    "        conv1 = conv(self.X, 11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "        norm1 = lrn(conv1, 2, 1e-05, 0.75, name='norm1')\n",
    "        pool1 = max_pool(norm1, 3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "        \n",
    "        # 2nd Layer: Conv (w ReLu)  -> Lrn -> Pool with 2 groups\n",
    "        conv2 = conv(pool1, 5, 5, 256, 1, 1, groups=2, name='conv2')\n",
    "        norm2 = lrn(conv2, 2, 1e-05, 0.75, name='norm2')\n",
    "        pool2 = max_pool(norm2, 3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "        \n",
    "        # 3rd Layer: Conv (w ReLu)\n",
    "        conv3 = conv(pool2, 3, 3, 384, 1, 1, name='conv3')\n",
    "\n",
    "        # 4th Layer: Conv (w ReLu) splitted into two groups\n",
    "        conv4 = conv(conv3, 3, 3, 384, 1, 1, groups=2, name='conv4')\n",
    "\n",
    "        # 5th Layer: Conv (w ReLu) -> Pool splitted into two groups\n",
    "        conv5 = conv(conv4, 3, 3, 256, 1, 1, groups=2, name='conv5')\n",
    "        pool5 = max_pool(conv5, 3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "\n",
    "        # 6th Layer: Flatten -> FC (w ReLu) -> Dropout\n",
    "        flattened = tf.reshape(pool5, [-1, 6*6*256])\n",
    "        fc6 = fc(flattened, 6*6*256, 4096, name='fc6')\n",
    "        dropout6 = dropout(fc6, self.KEEP_PROB)\n",
    "\n",
    "        # 7th Layer: FC (w ReLu) -> Dropout\n",
    "        fc7 = fc(dropout6, 4096, 4096, name='fc7')\n",
    "        dropout7 = dropout(fc7, self.KEEP_PROB)\n",
    "\n",
    "        # 8th Layer: FC and return unscaled activations\n",
    "        self.fc8 = fc(dropout7, 4096, self.NUM_CLASSES, relu=False, name='fc8')\n",
    "\n",
    "    def load_initial_weights(self, session):\n",
    "        \"\"\"Load weights from file into network.\n",
    "\n",
    "        As the weights from http://www.cs.toronto.edu/~guerzhoy/tf_alexnet/\n",
    "        come as a dict of lists (e.g. weights['conv1'] is a list) and not as\n",
    "        dict of dicts (e.g. weights['conv1'] is a dict with keys 'weights' &\n",
    "        'biases') we need a special load function\n",
    "        \"\"\"\n",
    "        # Load the weights into memory\n",
    "        weights_dict = np.load(self.WEIGHTS_PATH, encoding='bytes').item()\n",
    "\n",
    "        # Loop over all layer names stored in the weights dict\n",
    "        for op_name in weights_dict:\n",
    "\n",
    "            # Check if layer should be trained from scratch\n",
    "            if op_name not in self.SKIP_LAYER:\n",
    "\n",
    "                with tf.variable_scope(op_name, reuse=True):\n",
    "\n",
    "                    # Assign weights/biases to their corresponding tf variable\n",
    "                    for data in weights_dict[op_name]:\n",
    "\n",
    "                        # Biases\n",
    "                        if len(data.shape) == 1:\n",
    "                            var = tf.get_variable('biases', trainable=False)\n",
    "                            session.run(var.assign(data))\n",
    "\n",
    "                        # Weights\n",
    "                        else:\n",
    "                            var = tf.get_variable('weights', trainable=False)\n",
    "                            session.run(var.assign(data))\n",
    "\n",
    "\n",
    "def conv(x, filter_height, filter_width, num_filters, stride_y, stride_x, name,\n",
    "         padding='SAME', groups=1):\n",
    "    \"\"\"Create a convolution layer.\n",
    "\n",
    "    Adapted from: https://github.com/ethereon/caffe-tensorflow\n",
    "    \"\"\"\n",
    "    # Get number of input channels\n",
    "    input_channels = int(x.get_shape()[-1])\n",
    "\n",
    "    # Create lambda function for the convolution\n",
    "    convolve = lambda i, k: tf.nn.conv2d(i, k,\n",
    "                                         strides=[1, stride_y, stride_x, 1],\n",
    "                                         padding=padding)\n",
    "\n",
    "    with tf.variable_scope(name) as scope:\n",
    "        # Create tf variables for the weights and biases of the conv layer\n",
    "        weights = tf.get_variable('weights', shape=[filter_height,\n",
    "                                                    filter_width,\n",
    "                                                    input_channels/groups,\n",
    "                                                    num_filters])\n",
    "        biases = tf.get_variable('biases', shape=[num_filters])\n",
    "\n",
    "    if groups == 1:\n",
    "        conv = convolve(x, weights)\n",
    "\n",
    "    # In the cases of multiple groups, split inputs & weights and\n",
    "    else:\n",
    "        # Split input and weights and convolve them separately\n",
    "        input_groups = tf.split(axis=3, num_or_size_splits=groups, value=x)\n",
    "        weight_groups = tf.split(axis=3, num_or_size_splits=groups,\n",
    "                                 value=weights)\n",
    "        output_groups = [convolve(i, k) for i, k in zip(input_groups, weight_groups)]\n",
    "\n",
    "        # Concat the convolved output together again\n",
    "        conv = tf.concat(axis=3, values=output_groups)\n",
    "\n",
    "    # Add biases\n",
    "    bias = tf.reshape(tf.nn.bias_add(conv, biases), tf.shape(conv))\n",
    "\n",
    "    # Apply relu function\n",
    "    relu = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    return relu\n",
    "\n",
    "\n",
    "def fc(x, num_in, num_out, name, relu=True):\n",
    "    \"\"\"Create a fully connected layer.\"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "\n",
    "        # Create tf variables for the weights and biases\n",
    "        weights = tf.get_variable('weights', shape=[num_in, num_out],\n",
    "                                  trainable=True)\n",
    "        biases = tf.get_variable('biases', [num_out], trainable=True)\n",
    "\n",
    "        # Matrix multiply weights and inputs and add bias\n",
    "        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\n",
    "\n",
    "    if relu:\n",
    "        # Apply ReLu non linearity\n",
    "        relu = tf.nn.relu(act)\n",
    "        return relu\n",
    "    else:\n",
    "        return act\n",
    "\n",
    "\n",
    "def max_pool(x, filter_height, filter_width, stride_y, stride_x, name,\n",
    "             padding='SAME'):\n",
    "    \"\"\"Create a max pooling layer.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, filter_height, filter_width, 1],\n",
    "                          strides=[1, stride_y, stride_x, 1],\n",
    "                          padding=padding, name=name)\n",
    "\n",
    "\n",
    "def lrn(x, radius, alpha, beta, name, bias=1.0):\n",
    "    \"\"\"Create a local response normalization layer.\"\"\"\n",
    "    return tf.nn.local_response_normalization(x, depth_radius=radius,\n",
    "                                              alpha=alpha, beta=beta,\n",
    "                                              bias=bias, name=name)\n",
    "\n",
    "\n",
    "def dropout(x, keep_prob):\n",
    "    \"\"\"Create a dropout layer.\"\"\"\n",
    "    return tf.nn.dropout(x, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#placeholder for input and dropout rate\n",
    "x = tf.placeholder(tf.float32, [1, 227, 227, 3])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "#create model with default config ( == no skip_layer and 1000 units in the last layer)\n",
    "model = AlexNet(x, keep_prob, 1000, [])\n",
    "\n",
    "#define activation of last layer as score\n",
    "score = model.fc8\n",
    "\n",
    "#create op to calculate softmax \n",
    "softmax = tf.nn.softmax(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Load the pretrained weights into the model\n",
    "    model.load_initial_weights(sess)\n",
    "    \n",
    "    # Create figure handle\n",
    "    fig2 = plt.figure(figsize=(15,6))\n",
    "    \n",
    "    # Loop over all images\n",
    "    for i, image in enumerate(imgs):\n",
    "        \n",
    "        # Convert image to float32 and resize to (227x227)\n",
    "        img = cv2.resize(image.astype(np.float32), (227,227))\n",
    "        \n",
    "        # Subtract the ImageNet mean\n",
    "        img -= imagenet_mean\n",
    "        \n",
    "        # Reshape as needed to feed into model\n",
    "        img = img.reshape((1,227,227,3))\n",
    "        \n",
    "        # Run the session and calculate the class probability\n",
    "        probs = sess.run(softmax, feed_dict={x: img, keep_prob: 1})\n",
    "        \n",
    "        # Get the class name of the class with the highest probability\n",
    "        class_name = class_names[np.argmax(probs)]\n",
    "        \n",
    "        # Plot image with class name and prob in the title\n",
    "        fig2.add_subplot(1,3,i+1)\n",
    "        plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"Class: \" + class_name + \", probability: %.4f\" %probs[0,np.argmax(probs)])\n",
    "        plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
